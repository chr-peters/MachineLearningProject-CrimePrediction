\mode*

\begin{frame}
	\section{Data Preparation}
	\frametitle{Data Preparation}
        \begin{itemize}
          \item Before the classification can be done, a few steps are required on the raw data
          \item Original dataset does not contain the class labels ("High", "Medium", "Low")
            \begin{itemize}
              \item How to assign each instance a crime category based on its crime rate?
              \item How to find the percentage boundaries between the classes?
            \end{itemize}
          \item Curse of dimensionality
            \begin{itemize}
              \item Huge number of attributes (128 per city)
              \item Select only the most significant
              \item Reduce the dimensionality of the feature space
            \end{itemize}
        \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Assignment of Class Labels}
  \begin{itemize}
    \item Divide the attribute "crime rate" into different groups
      \begin{itemize}
        \item Initial question: How many groups?
        \item What are the boundaries between the groups?
      \end{itemize}
    \item Use clustering to solve this problem
      \begin{itemize}
        \item Simple k-Means clustering algorithm
        \item Only cluster the crime rate (i.e. only one dimension)
        \item Test different k values (different amounts of groups)
        \item Find the boundaries between the groups
      \end{itemize}
    \item Each cluster will be assigned a label (e.g. "High", "Medium", "Low",\ldots crime rate)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Assignment of Class Labels}
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.8\columnwidth]{../../charts/SSE.png}
    \caption{Sum of the squared errors for different choices of k}
    \label{fig:sse}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Assignment of Class Labels}
  \begin{itemize}
    \item By using the elbow-method, \(k=3\) is the best choice
      \begin{itemize}
        \item This means to divide the crime rate into three groups ("High", "Medium", "Low")
      \end{itemize}
    \item The percentage boundaries are extracted from the clusters
      \begin{itemize}
        \item Low: \([0\%; 22\%]\) 
	\item Medium: \((22\%; 56\%]\)
	\item High: \((56\%; 100\%]\)
      \end{itemize}
    \item Depending on the group it falls into, each instance (city) can now be assigned a label
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Feature Selection}
  \begin {itemize}
    \item Two-step approach:
      \begin{enumerate}
        \item Create a ranking by judgement of significance
          \begin{itemize}
            \item Discuss each attribute
            \item Agree if it is included or not
          \end{itemize}
        \item Generate a ranking based on the correlation between each attribute and the class labels
      \end{enumerate}
    \item Compare the judgement ranking with the correlation ranking
    \item Add some features with high correlation and remove some with low correlation
      \begin{itemize}
        \item E.g. the attribute "population" was removed because of a correlation near zero
      \end{itemize}
  \end{itemize}
\end{frame}


Before the classification can be done, a few preparation steps are still
required, namely the assignment of class labels to each instance as
well as feature selection to reduce the set of attributes to an
acceptable size.

\subsection{Assignment of Class Labels}
\label{sec:assignment}

To obtain the class labels used for classification, the
continuous attribute "total number of violent crimes per 100K
population" is divided into separate groups. Each of this groups is
assigned an ordinal class label that can be predicted during classification.
If for example the number of groups is equal to three, one could
assign the labels "High", "Medium" and "Low" to the corresponding
groups.

The simple k-Means clustering algorithm is used to achieve this
by comparing different amounts of groups, i.e. testing
different values of k. Those different k values were compared by using
the elbow-method and \(k=3\) was revealed as the best parameter as
shown in \fref{fig:sse}.
\begin{figure}[H]
	\centering
	\includegraphics[width=\columnwidth]{../../charts/SSE.png}
	\caption{Sum of the squared errors for different choices of k}
	\label{fig:sse}
\end{figure}
\noindent The three classes originating from this choice are namely
"High", "Medium" and "Low", the percentage boundaries are
respectively:
\begin{description}
	\setlength{\itemsep}{-2pt}
	\item[Low:] \([0\%; 22\%]\) 
	\item[Medium:] \((22\%; 56\%]\)
	\item[High:] \((56\%; 100\%]\)
\end{description}
%% It should be noted, that the authors of \textit{"An Experimental Study
%% of Classification Algorithms for Crime Prediction"} \cite{indian}
%% obtained different results for the percentage boundaries, they did
%% however not disclose the details of their approach.
%% Their percentage boundaries are "Low": \([0\%;25\%)\), "Medium": \([25\%; 40\%)\) and "High": \([40\%; 100\%]\).

\subsection{Feature Selection}
\label{sec:feature_selection}    

To reduce the total of 128 attributes in the dataset to an acceptable
amount, we employed a two-step approach that is presented in the
following sections.

\paragraph{Selection by Judgement of Significance}
First we discussed each attribute separately and agreed whether we
would include it based on its significance. In the course of this
procedure, we obtained a ranking of the attributes we regarded as most
significant.

\paragraph{Comparison with Correlation Ranking}
In the second step, we compared our choices with the attributes that
have the highest correlation with the classes. We found that the
majority of the attributes that we selected based on common sense was
already present in the ranking, however a few attributes like
"population" were discarded because of a correlation near zero.


\mode<all>
